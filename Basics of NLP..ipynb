{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396a231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"In this course you will learn what Artificial Intelligence (AI) is, explore use cases and applications of AI, \n",
    "understand AI concepts and terms like machine learning, deep learning and neural networks. You will be exposed to various \n",
    "issues and concerns surrounding AI such as ethics and bias, & jobs, and get advice from experts about learning and starting \n",
    "a career in AI.  You will also demonstrate AI in action with a mini project.\n",
    "This course does not require any programming or computer science expertise and is desi Artificial intelligence was founded as \n",
    "an academic discipline in 1956, and in the years since has experienced several waves of optimism, [6] [7] followed by \n",
    "disappointment and the loss of funding (known as an \"AI winter\"), [8] [9] followed by new approaches, success and renewed \n",
    "funding. [7] [10] AI\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b23ce9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this course you will learn what Artificial Intelligence (AI) is, explore use cases and applications of AI, \\nunderstand AI concepts and terms like machine learning, deep learning and neural networks. You will be exposed to various \\nissues and concerns surrounding AI such as ethics and bias, & jobs, and get advice from experts about learning and starting \\na career in AI.  You will also demonstrate AI in action with a mini project.\\nThis course does not require any programming or computer science expertise and is desi Artificial intelligence was founded as \\nan academic discipline in 1956, and in the years since has experienced several waves of optimism, [6] [7] followed by \\ndisappointment and the loss of funding (known as an \"AI winter\"), [8] [9] followed by new approaches, success and renewed \\nfunding. [7] [10] AI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e1432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Library.\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67bd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the paragrap into sentance.\n",
    "sentence = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee316b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this course you will learn what Artificial Intelligence (AI) is, explore use cases and applications of AI, \\nunderstand AI concepts and terms like machine learning, deep learning and neural networks.',\n",
       " 'You will be exposed to various \\nissues and concerns surrounding AI such as ethics and bias, & jobs, and get advice from experts about learning and starting \\na career in AI.',\n",
       " 'You will also demonstrate AI in action with a mini project.',\n",
       " 'This course does not require any programming or computer science expertise and is desi Artificial intelligence was founded as \\nan academic discipline in 1956, and in the years since has experienced several waves of optimism, [6] [7] followed by \\ndisappointment and the loss of funding (known as an \"AI winter\"), [8] [9] followed by new approaches, success and renewed \\nfunding.',\n",
       " '[7] [10] AI']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061f32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAke a Tokenice the words.\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf918dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'course',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'what',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " ',',\n",
       " 'explore',\n",
       " 'use',\n",
       " 'cases',\n",
       " 'and',\n",
       " 'applications',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'understand',\n",
       " 'AI',\n",
       " 'concepts',\n",
       " 'and',\n",
       " 'terms',\n",
       " 'like',\n",
       " 'machine',\n",
       " 'learning',\n",
       " ',',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'neural',\n",
       " 'networks',\n",
       " '.',\n",
       " 'You',\n",
       " 'will',\n",
       " 'be',\n",
       " 'exposed',\n",
       " 'to',\n",
       " 'various',\n",
       " 'issues',\n",
       " 'and',\n",
       " 'concerns',\n",
       " 'surrounding',\n",
       " 'AI',\n",
       " 'such',\n",
       " 'as',\n",
       " 'ethics',\n",
       " 'and',\n",
       " 'bias',\n",
       " ',',\n",
       " '&',\n",
       " 'jobs',\n",
       " ',',\n",
       " 'and',\n",
       " 'get',\n",
       " 'advice',\n",
       " 'from',\n",
       " 'experts',\n",
       " 'about',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'starting',\n",
       " 'a',\n",
       " 'career',\n",
       " 'in',\n",
       " 'AI',\n",
       " '.',\n",
       " 'You',\n",
       " 'will',\n",
       " 'also',\n",
       " 'demonstrate',\n",
       " 'AI',\n",
       " 'in',\n",
       " 'action',\n",
       " 'with',\n",
       " 'a',\n",
       " 'mini',\n",
       " 'project',\n",
       " '.',\n",
       " 'This',\n",
       " 'course',\n",
       " 'does',\n",
       " 'not',\n",
       " 'require',\n",
       " 'any',\n",
       " 'programming',\n",
       " 'or',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'expertise',\n",
       " 'and',\n",
       " 'is',\n",
       " 'desi',\n",
       " 'Artificial',\n",
       " 'intelligence',\n",
       " 'was',\n",
       " 'founded',\n",
       " 'as',\n",
       " 'an',\n",
       " 'academic',\n",
       " 'discipline',\n",
       " 'in',\n",
       " '1956',\n",
       " ',',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'years',\n",
       " 'since',\n",
       " 'has',\n",
       " 'experienced',\n",
       " 'several',\n",
       " 'waves',\n",
       " 'of',\n",
       " 'optimism',\n",
       " ',',\n",
       " '[',\n",
       " '6',\n",
       " ']',\n",
       " '[',\n",
       " '7',\n",
       " ']',\n",
       " 'followed',\n",
       " 'by',\n",
       " 'disappointment',\n",
       " 'and',\n",
       " 'the',\n",
       " 'loss',\n",
       " 'of',\n",
       " 'funding',\n",
       " '(',\n",
       " 'known',\n",
       " 'as',\n",
       " 'an',\n",
       " '``',\n",
       " 'AI',\n",
       " 'winter',\n",
       " \"''\",\n",
       " ')',\n",
       " ',',\n",
       " '[',\n",
       " '8',\n",
       " ']',\n",
       " '[',\n",
       " '9',\n",
       " ']',\n",
       " 'followed',\n",
       " 'by',\n",
       " 'new',\n",
       " 'approaches',\n",
       " ',',\n",
       " 'success',\n",
       " 'and',\n",
       " 'renewed',\n",
       " 'funding',\n",
       " '.',\n",
       " '[',\n",
       " '7',\n",
       " ']',\n",
       " '[',\n",
       " '10',\n",
       " ']',\n",
       " 'AI']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a40987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a stemmer variable.\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521ff60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56bcc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Lematization Libaary.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer  = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57955bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa14937d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drinking'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('drinking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d688b0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " lemmatizer.lemmatize('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e3fc577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc908eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "444e350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convrt the uppercase into lowercase text.\n",
    "corps = []\n",
    "for i in range(len(sentence)):\n",
    "    review = re.sub('[^a-zA-z]',' ',sentence[i])\n",
    "    review = review.lower()\n",
    "    corps.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69df3124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in this course you will learn what artificial intelligence  ai  is  explore use cases and applications of ai   understand ai concepts and terms like machine learning  deep learning and neural networks ',\n",
       " 'you will be exposed to various  issues and concerns surrounding ai such as ethics and bias    jobs  and get advice from experts about learning and starting  a career in ai ',\n",
       " 'you will also demonstrate ai in action with a mini project ',\n",
       " 'this course does not require any programming or computer science expertise and is desi artificial intelligence was founded as  an academic discipline in       and in the years since has experienced several waves of optimism  [ ] [ ] followed by  disappointment and the loss of funding  known as an  ai winter    [ ] [ ] followed by new approaches  success and renewed  funding ',\n",
       " '[ ] [  ] ai']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd8e598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "thi\n",
      "cours\n",
      "you\n",
      "will\n",
      "learn\n",
      "what\n",
      "artifici\n",
      "intellig\n",
      "ai\n",
      "is\n",
      "explor\n",
      "use\n",
      "case\n",
      "and\n",
      "applic\n",
      "of\n",
      "ai\n",
      "understand\n",
      "ai\n",
      "concept\n",
      "and\n",
      "term\n",
      "like\n",
      "machin\n",
      "learn\n",
      "deep\n",
      "learn\n",
      "and\n",
      "neural\n",
      "network\n",
      "you\n",
      "will\n",
      "be\n",
      "expos\n",
      "to\n",
      "variou\n",
      "issu\n",
      "and\n",
      "concern\n",
      "surround\n",
      "ai\n",
      "such\n",
      "as\n",
      "ethic\n",
      "and\n",
      "bia\n",
      "job\n",
      "and\n",
      "get\n",
      "advic\n",
      "from\n",
      "expert\n",
      "about\n",
      "learn\n",
      "and\n",
      "start\n",
      "a\n",
      "career\n",
      "in\n",
      "ai\n",
      "you\n",
      "will\n",
      "also\n",
      "demonstr\n",
      "ai\n",
      "in\n",
      "action\n",
      "with\n",
      "a\n",
      "mini\n",
      "project\n",
      "thi\n",
      "cours\n",
      "doe\n",
      "not\n",
      "requir\n",
      "ani\n",
      "program\n",
      "or\n",
      "comput\n",
      "scienc\n",
      "expertis\n",
      "and\n",
      "is\n",
      "desi\n",
      "artifici\n",
      "intellig\n",
      "wa\n",
      "found\n",
      "as\n",
      "an\n",
      "academ\n",
      "disciplin\n",
      "in\n",
      "and\n",
      "in\n",
      "the\n",
      "year\n",
      "sinc\n",
      "ha\n",
      "experienc\n",
      "sever\n",
      "wave\n",
      "of\n",
      "optim\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "follow\n",
      "by\n",
      "disappoint\n",
      "and\n",
      "the\n",
      "loss\n",
      "of\n",
      "fund\n",
      "known\n",
      "as\n",
      "an\n",
      "ai\n",
      "winter\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "follow\n",
      "by\n",
      "new\n",
      "approach\n",
      "success\n",
      "and\n",
      "renew\n",
      "fund\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "ai\n"
     ]
    }
   ],
   "source": [
    "#applaying stemmer for each words.\n",
    "for i in corps:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e326d8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf3cff8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62a45388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "397945e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cours\n",
      "learn\n",
      "artifici\n",
      "intellig\n",
      "ai\n",
      "explor\n",
      "use\n",
      "case\n",
      "applic\n",
      "ai\n",
      "understand\n",
      "ai\n",
      "concept\n",
      "term\n",
      "like\n",
      "machin\n",
      "learn\n",
      "deep\n",
      "learn\n",
      "neural\n",
      "network\n",
      "expos\n",
      "variou\n",
      "issu\n",
      "concern\n",
      "surround\n",
      "ai\n",
      "ethic\n",
      "bia\n",
      "job\n",
      "get\n",
      "advic\n",
      "expert\n",
      "learn\n",
      "start\n",
      "career\n",
      "ai\n",
      "also\n",
      "demonstr\n",
      "ai\n",
      "action\n",
      "mini\n",
      "project\n",
      "cours\n",
      "requir\n",
      "program\n",
      "comput\n",
      "scienc\n",
      "expertis\n",
      "desi\n",
      "artifici\n",
      "intellig\n",
      "found\n",
      "academ\n",
      "disciplin\n",
      "year\n",
      "sinc\n",
      "experienc\n",
      "sever\n",
      "wave\n",
      "optim\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "follow\n",
      "disappoint\n",
      "loss\n",
      "fund\n",
      "known\n",
      "ai\n",
      "winter\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "follow\n",
      "new\n",
      "approach\n",
      "success\n",
      "renew\n",
      "fund\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "ai\n"
     ]
    }
   ],
   "source": [
    "#appalaying the stop words for each words.\n",
    "for i in corps:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(stemmer.stem(word))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "979d27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course\n",
      "learn\n",
      "artificial\n",
      "intelligence\n",
      "ai\n",
      "explore\n",
      "use\n",
      "case\n",
      "application\n",
      "ai\n",
      "understand\n",
      "ai\n",
      "concept\n",
      "term\n",
      "like\n",
      "machine\n",
      "learning\n",
      "deep\n",
      "learning\n",
      "neural\n",
      "network\n",
      "exposed\n",
      "various\n",
      "issue\n",
      "concern\n",
      "surrounding\n",
      "ai\n",
      "ethic\n",
      "bias\n",
      "job\n",
      "get\n",
      "advice\n",
      "expert\n",
      "learning\n",
      "starting\n",
      "career\n",
      "ai\n",
      "also\n",
      "demonstrate\n",
      "ai\n",
      "action\n",
      "mini\n",
      "project\n",
      "course\n",
      "require\n",
      "programming\n",
      "computer\n",
      "science\n",
      "expertise\n",
      "desi\n",
      "artificial\n",
      "intelligence\n",
      "founded\n",
      "academic\n",
      "discipline\n",
      "year\n",
      "since\n",
      "experienced\n",
      "several\n",
      "wave\n",
      "optimism\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "followed\n",
      "disappointment\n",
      "loss\n",
      "funding\n",
      "known\n",
      "ai\n",
      "winter\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "followed\n",
      "new\n",
      "approach\n",
      "success\n",
      "renewed\n",
      "funding\n",
      "[\n",
      "]\n",
      "[\n",
      "]\n",
      "ai\n"
     ]
    }
   ],
   "source": [
    "#Lematization\n",
    "for i in corps:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "822f1dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c031dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = cv.fit_transform(corps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c1cc41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'this', 'course', 'you', 'will', 'learn', 'what', 'artificial', 'intelligence', 'ai', 'is', 'explore', 'use', 'cases', 'and', 'applications', 'of', 'understand', 'concepts', 'terms', 'like', 'machine', 'learning', 'deep', 'neural', 'networks', 'be', 'exposed', 'to', 'various', 'issues', 'concerns', 'surrounding', 'such', 'as', 'ethics', 'bias', 'jobs', 'get', 'advice', 'from', 'experts', 'about', 'starting', 'career', 'also', 'demonstrate', 'action', 'with', 'mini', 'project', 'does', 'not', 'require', 'any', 'programming', 'or', 'computer', 'science', 'expertise', 'desi', 'was', 'founded', 'an', 'academic', 'discipline', 'the', 'years', 'since', 'has', 'experienced', 'several', 'waves', 'optimism', 'followed', 'by', 'disappointment', 'loss', 'funding', 'known', 'winter', 'new', 'approaches', 'success', 'renewed']\n"
     ]
    }
   ],
   "source": [
    "print(list(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e248dff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6ea1a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you will also demonstrate ai in action with a mini project '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corps[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cbf820d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f131ad51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in this course you will learn what artificial intelligence  ai  is  explore use cases and applications of ai   understand ai concepts and terms like machine learning  deep learning and neural networks ',\n",
       " 'you will be exposed to various  issues and concerns surrounding ai such as ethics and bias    jobs  and get advice from experts about learning and starting  a career in ai ',\n",
       " 'you will also demonstrate ai in action with a mini project ',\n",
       " 'this course does not require any programming or computer science expertise and is desi artificial intelligence was founded as  an academic discipline in       and in the years since has experienced several waves of optimism  [ ] [ ] followed by  disappointment and the loss of funding  known as an  ai winter    [ ] [ ] followed by new approaches  success and renewed  funding ',\n",
       " '[ ] [  ] ai']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d5f40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "af = TfidfVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b4afd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = af.fit_transform(corps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47840be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this course you will learn what artificial intelligence  ai  is  explore use cases and applications of ai   understand ai concepts and terms like machine learning  deep learning and neural networks '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a66d0536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.18569534,\n",
       "        0.        , 0.18569534, 0.        , 0.18569534, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.18569534, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18569534,\n",
       "        0.        , 0.        , 0.18569534, 0.        , 0.        ,\n",
       "        0.18569534, 0.        , 0.18569534, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.18569534, 0.        , 0.18569534,\n",
       "        0.        , 0.        , 0.18569534, 0.18569534, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.18569534, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.18569534, 0.18569534, 0.        , 0.        , 0.18569534,\n",
       "        0.        , 0.        , 0.        , 0.18569534, 0.18569534,\n",
       "        0.        , 0.18569534, 0.18569534, 0.        , 0.18569534,\n",
       "        0.        , 0.        , 0.18569534, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.18569534, 0.        , 0.        , 0.        ,\n",
       "        0.18569534, 0.        , 0.18569534, 0.18569534, 0.        ,\n",
       "        0.        , 0.        , 0.18569534, 0.        , 0.        ,\n",
       "        0.18569534, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.18569534]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ffd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
